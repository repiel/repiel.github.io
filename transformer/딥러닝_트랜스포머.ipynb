{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "177f8907",
   "metadata": {},
   "source": [
    "- 이 내용은 아래의 딥러닝 논문 리뷰 유튜브를 참고하여 만들어짐\n",
    "- https://youtu.be/AA621UofTUA "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f37b40",
   "metadata": {},
   "source": [
    "# 딥러닝 기반의 기계 번역 발전 과정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c080ae",
   "metadata": {},
   "source": [
    "## Attention부터 입력 시퀀스 전체에서 정보를 추출하는 방향으로 발전"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca23deaa",
   "metadata": {},
   "source": [
    "- GPT: Transformer의 디코더(Decoder) 아키텍처를 활용\n",
    "- BERT: Transformer의 인코더(Encoder) 아키텍처를 활용\n",
    "- RNN(1986) - LSTM(1997) - Seq2Seq(NIPS 2014)[여기까지 고정된 크기의 context vector 사용]\n",
    "- Attention(ICLR 2015) - Transformer(NIPS 2017) - GPT-1(2018) - BERT(NAACL 2019) - GPT-3(2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf20836",
   "metadata": {},
   "source": [
    "## 기존 Seq2Seq 모델의 한계점"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22046c7e",
   "metadata": {},
   "source": [
    "- context vector에 소스문장의 정보를 압축\n",
    "  <br>- 그 때문에 많은 양의 정보를 벡터에 압축하였다가 풀어지는 과정에서 병목 현상이 발생하여 성능 하락의 원인이 됨."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbbfc95",
   "metadata": {},
   "source": [
    "- 디코더가 context vector를 매번 참고하게 할 수도 있음.\n",
    "<br>- 하지만 그것도 여전히 소스 문장을 하나의 벡터에 압축해야 하는 불편함을 가지고 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c733a1de",
   "metadata": {},
   "source": [
    "## Seq2Seq 모델 + Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d9936b",
   "metadata": {},
   "source": [
    "- Seq2Seq 모델의 문제: 하나의 문맥 벡터가 소스 문장의 모든 정보를 가지고 있어야 하기 때문에 성능이 저하된다.\n",
    "- 이를 해결하기 위해선 매번 소스 문장에서의 출력 전부를 입력받으면 된다!\n",
    "- 그래서 만들어진 것이 Seq2Seq 모델 + Attention이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc0b5d8",
   "metadata": {},
   "source": [
    "### 특징\n",
    "- 디코더는 인코더의 모든 출력을 참고한다\n",
    "- 디코더는 매번 인코더의 모든 출력 중에서 어떤 정보가 중요한지를 계산\n",
    "(에너지와 가중치를 고려하여 우선순위를 정한다고 생각하면 이해아기 쉬움)\n",
    "- 시각화각 가능하여 Attention 가중치를 사용해 각 출력이 어떤 입력 정보를 참고했는지 알 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47525ec2",
   "metadata": {},
   "source": [
    "# 트랜스포머 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b899f21",
   "metadata": {},
   "source": [
    "- 논문의 제목이 Attention Is All You Need인 만큼 Attention의 중요성이 강조된다.\n",
    "- RNN이나 CNN을 전혀 필요로 하지 않는다.\n",
    "- - 대신 Positional Encoding을 사용한다.\n",
    "- BERT와 같은 향상된 네트워크에서도 채택되고 있다.\n",
    "- 인코더와 디코더로 구성이 되며 Attention과정을 여러 레이어에서 반복한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b7e785",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725a5dff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8011ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
